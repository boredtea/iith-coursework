{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ES19BTECH11017_Assign1","provenance":[{"file_id":"1Y3ThP9SWSlzZIWU0g4EhGnxgElCk_FLz","timestamp":1613735446560}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YK_030GdmnpC"},"source":["# Question 1\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1cxZatB7PW8g"},"source":["## (a)\n","\n","The training error will increase as k varies from 1 to n, however, this increase isn't linear. The training error when k = 1 is going to be zero because each time the nearest neighbour of a node will be the point itself since its distance from itself will be zero, and the error will be the highest when k = n because the answer will remain the same for all data points.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Z0IGR6S2PdTG"},"source":["## (b)\n","\n","The generaisation error will decrease and then increase from k = 1 to n. Due to the overlapping nature of the data points, when k = 1 or a smaller number, there's a high possibility of the final class getting influenced by nearby noise points. As k gets larger, the error will reduce because the k nearest neighbours will start to return more accurate results. However, as k approaches n, the error increases again because the output can be either class with a 50/50 probability. The graph can be found [here](https://drive.google.com/file/d/1527x8CRLY_hj3TMJWodpNKPR-awPWSlq/view?usp=sharing).\n","\n","<!-- <p><a href=\"https://drive.google.com/file/d/1527x8CRLY_hj3TMJWodpNKPR-awPWSlq/view?usp=sharing\"><img src=\"https://drive.google.com/file/d/1527x8CRLY_hj3TMJWodpNKPR-awPWSlq/view?usp=sharing\"></a></p> -->\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hz_kfVejPfbV"},"source":["## (c)\n","\n","k-NN is an undesireable algorithm when the dimensionality of the input is high because:\n","* As mentioned in class, all the points lie on the surface of a unit hypersphere, which implies that the Eucledian distance between the query point and the other data points start to become very close in value, i.e., the closest and the farthest points have the same distance, and the term \"nearest neighbours\" starts to lose sense. Hence, choosing the k nearest neighbours based on distance makes no sense in higher dimensions and we start to get erroneous outputs\n","* Calculating distances in a Euclidean space makes sense for smaller dimensions but as we move to higher dimensions, it would make more sense to do the calculations in a probabilistic space\n","* In higher dimensions, the k-NN algorithm will also start to get more computationally expensive because calculating the Eucledean distance might become a large task which will increase computation time\n","\n"]},{"cell_type":"markdown","metadata":{"id":"W_JshhopPg3e"},"source":["## (d)\n","\n","No, a univariate decision tree cannot function exactly like a 1-NN. To justify this, we must take into conosideration the Voronoi diagram described in class. It divides the 2D plane into regions whose edges are the decision boundaries obtained from the training examples. It is evident from the diagram that the edges can be in any direction and don't necessarily have to be parallel to the 2D axes. However, with a decision tree of the given format, all the decision boundaries will be parallel to the axes. This is clearly very different from the Voronoi diagram and will incorrectly classify a lot of points, and hence it cannot be used to function like a 1-NN."]},{"cell_type":"markdown","metadata":{"id":"vqbt8gVDjtmn"},"source":["# Question 2\n","\n","## (a)\n","\n","In the given question, to fit a Gaussian to the two classes using maximum likelihood, we just need to find $\\mu$ which is just the mean of the given data. The fact that $\\mu$ will be the mean of the given data can be easily proved by performing the following steps:\n","\n","1. Take the logarithm of the maximum likelihood formula\n","2. Differentiate the given formula with respect to $\\mu$\n","3. Set the above equation to zero since we need to maximise the likelihood and solve for $\\mu$\n","4. Solving for $\\mu$ will lead us to the answer which turns out to be the mean of the given data\n","\n","$\\mu = \\frac{\\sum_{i=1}^{n} x_i}{n}$\n","Note: the sigma values have been directly copied from the assignment document.\n","\n","\n","Thus, for class 1:\\\n","$$\n","\\mu = 0.26 \\\\\n","\\sigma^2 = 0.0149\n","$$\\\n","And for class 2:\\\n","$$\n","\\mu = 0.8625 \\\\\n","\\sigma^2 = 0.0092\n","$$\n","\n","Class probability is the probability of a point belonging to either class.\n","$$\n","p(c_i) = \\frac{N_i}{\\sum_{j=1}^{n} N_j}\n","$$\n","\n","Where, $c_i$ is the class $i$ and $N_i$ is the size of $c_i$.\n","\n","Thus, for class 1:\\\n","$$\n","p(c_1) = \\frac{10}{10+4}\\\\\n","p(c_1) = 0.714\n","$$\\\n","And for class 2:\\\n","$$\n","p(c_2) = \\frac{4}{10+4}\\\\\n","p(c_2) = 0.286\n","$$\n","\n","The Bayes' theorem:\n","$$\n","p(c_i|x) = \\frac{p(x|c_i)p(c_i)}{p(x)}\\\\\n","p(c_i|x) = \\frac{p(x|c_i)p(c_i)}{\\sum_{j=1}^{n} p(x|c_j)p(c_j)}\n","$$\n","\n","And the Gaussian formula:\n","$$\n","p(x|c_i) = \\frac{1}{\\sqrt{2\\pi \\sigma_i^{2}}} \\exp \\left(-\\frac{(x -\\mu_i)^2}{2\\sigma_i^2} \\right)\n","$$\n","\n","Thus for $p(c_1|0.6)$:\n","$$\n","p(c_1|0.6) = \\frac{p(0.6|c_1)p(c_1)}{p(0.6|c_1)p(c_1)+p(0.6|c_2)p(c_2)}\\\\\n","\\\\\n","p(0.6|c_1) = \\frac{1}{\\sqrt{2\\pi * 0.0149}} \\exp \\left(-\\frac{(0.6 - 0.26)^2}{2*0.0149} \\right) = 0.0675\\\\\n","p(0.6|c_2) = \\frac{1}{\\sqrt{2\\pi * 0.0092}} \\exp \\left(-\\frac{(0.6 - 0.8625)^2}{2*0.0092} \\right) = 0.0983\\\\\n","p(c_1|0.6) = 0.6316\n","$$\n","\n","                                                               \n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2GBq5H8BPmZM"},"source":["## (b)\n","\n","### Training Examples\n","| goal | football | golf | defence | offence | wicket | office | strategy | Sports or Not? |\n","|------|----------|------|---------|---------|--------|--------|----------|----------------|\n","|    1 |        0 |    1 |       1 |       1 |      0 |      1 |        1 |              0 |\n","|    0 |        0 |    0 |       1 |       0 |      0 |      1 |        1 |              0 |\n","|    1 |        0 |    0 |       1 |       1 |      0 |      1 |        0 |              0 |\n","|    0 |        1 |    0 |       0 |       1 |      1 |      0 |        1 |              0 |\n","|    0 |        0 |    0 |       1 |       1 |      0 |      1 |        1 |              0 |\n","|    0 |        0 |    0 |       1 |       1 |      0 |      0 |        1 |              0 |\n","|    1 |        1 |    0 |       0 |       0 |      0 |      0 |        0 |              1 |\n","|    0 |        0 |    1 |       0 |       0 |      0 |      0 |        0 |              1 |\n","|    1 |        1 |    0 |       1 |       0 |      0 |      0 |        0 |              1 |\n","|    1 |        1 |    0 |       1 |       0 |      0 |      0 |        1 |              1 |\n","|    1 |        1 |    0 |       1 |       1 |      0 |      0 |        0 |              1 |\n","|    0 |        0 |    0 |       1 |       0 |      1 |      0 |        0 |              1 |\n","\n","### Look Up Tables\n","\n","| politics                        \t| sports                          \t|\n","|---------------------------------\t|---------------------------------\t|\n","| <img width=250/>                  | <img width=250/>                  |\n","| $p(goal=1|sports=0) = 1/3$     \t| $p(goal=1|sports=1) = 2/3$     \t|\n","| $p(goal=0|sports=0) = 2/3$     \t| $p(goal=0|sports=1) = 1/3$     \t|\n","| $p(football=1|sports=0) = 1/6$ \t| $p(football=1|sports=1) = 2/3$ \t|\n","| $p(football=0|sports=0) = 5/6$ \t| $p(football=0|sports=1) = 1/3$ \t|\n","| $p(golf=1|sports=0) = 1/6$     \t| $p(golf=1|sports=1) = 1/6$     \t|\n","| $p(golf=0|sports=0) = 5/6$     \t| $p(golf=0|sports=1) = 5/6$     \t|\n","| $p(defence=1|sports=0) = 5/6$  \t| $p(defence=1|sports=1) = 2/3$  \t|\n","| $p(defence=0|sports=0) = 1/6$  \t| $p(defence=0|sports=1) = 1/3$  \t|\n","| $p(offence=1|sports=0) = 5/6$  \t| $p(offence=1|sports=1) = 1/6$  \t|\n","| $p(offence=0|sports=0) = 1/6$  \t| $p(offence=0|sports=1) = 5/6$  \t|\n","| $p(wicket=1|sports=0) = 1/6$   \t| $p(wicket=1|sports=1) = 1/6$   \t|\n","| $p(wicket=0|sports=0) = 5/6$   \t| $p(wicket=0|sports=1) = 5/6$   \t|\n","| $p(office=1|sports=0) = 2/3$   \t| $p(office=1|sports=1) = 0/1$   \t|\n","| $p(office=0|sports=0) = 1/3$   \t| $p(office=0|sports=1) = 1/1$   \t|\n","| $p(strategy=1|sports=0) = 5/6$ \t| $p(strategy=1|sports=1) = 1/6$ \t|\n","| $p(strategy=0|sports=0) = 1/6$ \t| $p(strategy=0|sports=1) = 5/6$ \t|\n","\n","\n","\n","$$\n","p(politics) = \\frac{6}{12} = 0.5\\\\\n","p(sports) = \\frac{6}{12} = 0.5\n","$$\n","\n","\n","Given,\\\n","$x = (1, 0, 0, 1, 1, 1, 1, 0)$\n","\n","$\n","p(politics|x) = \\frac{p(x|sports=0)p(politics)}{p(x|sports=0)p(politics) + p(x|sports=1)p(sports)}\\\\\n","$\\\n","Now,\\\n","$\n","p(x|sports=0)p(politics) = p(goal=1|sports=0) \\times p(football=0|sports=0) \\times p(golf=0|sports=0) \\times p(defence=1|sports=0) \\times p(offence=1|sports=0) \\times p(wicket=1|sports=0) \\times p(office=1|sports=0) \\times p(strategy=0|sports=0) \\times p(politics)\n","$\\\n","$p(x|sports=0) = 0.00149$\n","\n","\n","To find the probability, we need to find $p(x|sports=1)$.\n","\n","Now,\\\n","$\n","p(x|sports=1)p(sports) = p(goal=1|sports=1) \\times p(football=0|sports=1) \\times p(golf=0|sports=1) \\times p(defence=1|sports=1) \\times p(offence=1|sports=1) \\times p(wicket=1|sports=1) \\times p(office=1|sports=1) \\times p(strategy=0|sports=1) \\times p(sports)\n","$\\\n","\n","$p(x|sports=1) = 0.0$\n","\n","Thus,\n","$\n","p(politics|x) = \\frac{0.00149}{0.00149 + 0.0} = 1\n","$\n"]},{"cell_type":"markdown","metadata":{"id":"f1xgoA0nhfoj"},"source":["# Question 3"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TYXOTo5DhvWr","executionInfo":{"status":"ok","timestamp":1632496304025,"user_tz":-330,"elapsed":28643,"user":{"displayName":"SOUMI CHAKRABORTY","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09195013844947693927"}},"outputId":"8ec95502-6bcd-4820-9a64-3a2079299059"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"3A_FWn6khh2L"},"source":["## (a)"]},{"cell_type":"code","metadata":{"id":"L6rMpe7lhkNC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632496318656,"user_tz":-330,"elapsed":10296,"user":{"displayName":"SOUMI CHAKRABORTY","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09195013844947693927"}},"outputId":"4797e8bd-4031-40cd-fa32-fdf8420663f3"},"source":["# FoML Assign 1 Code Skeleton\n","# Please use this outline to implement your decision tree. You can add any code around this.\n","\n","import csv\n","import numpy as np\n","\n","# Enter You Name Here\n","myname = \"Soumi-Chakraborty\"\n","\n","class Node():\n","  def __init__(self, left=None, right=None, attribute_index=None, threshold=None, label=None, isLeaf=False):\n","    self.left = left\n","    self.right = right\n","    self.attribute_index = attribute_index\n","    self.threshold = threshold\n","    self.label = label\n","    self.isLeaf = isLeaf\n","\n","# Implement your decision tree below\n","class DecisionTree():\n","\n","    def getInfoGain(self, node, left, right):\n","      infoGain = self.entropy(node) - (np.shape(left)[0] / np.shape(node)[0]) * self.entropy(left) - (np.shape(right)[0] / np.shape(node)[0]) * self.entropy(right)  \n","      return infoGain\n","\n","    def entropy(self, y):\n","      #  since type can only be 0 or 1\n","      p0 = np.shape(y[y == 0])[0]/np.shape(y)[0]\n","      p1 = np.shape(y[y == 1])[0]/np.shape(y)[0]\n","      if p0 == 0 or p1 == 0:\n","        return 0\n","      entropy = -p0 * np.log2(p0) - p1 * np.log2(p1)\n","      return entropy\n","      \n","\n","    def learn(self, training_set):\n","        # implement this function\n","        X = training_set[:, :-1] # all rows, all columns except the last\n","        y = training_set[:, -1]  # all rows, only the last column\n","        y = y.reshape(y.shape[0], 1)\n","\n","        # purity check\n","        check = y[y == 1].shape[0]\n","        \n","        # split if max depth not reached\n","        if check != 0 and check != y.shape[0]:\n","          # get best split\n","          splitDetails = {\n","              \"infoGain\" : None, \n","              \"leftTrainingSet\": None, \n","              \"rightTrainingSet\": None, \n","              \"attributeIndex\" : None, \n","              \"threshold\": None\n","          }\n","          \n","          # check over all the attributes\n","          bestInfoGain = -float(\"inf\")\n","          for feature in range(X.shape[1]):\n","            values = training_set[:, feature] \n","            thersholds = np.unique(values)\n","            # compare all possible splits and choose the best one\n","            for thershold in thersholds:\n","              left_training_set = training_set[np.where(training_set[:, feature] <= thershold)]\n","              right_training_set = training_set[np.where(training_set[:, feature] > thershold)]\n","              if np.shape(left_training_set)[0] > 0 and np.shape(right_training_set)[0] > 0:\n","                yLeft = left_training_set[:, -1].reshape(np.shape(left_training_set)[0], 1)\n","                yRight = right_training_set[:, -1].reshape(np.shape(right_training_set)[0], 1)\n","\n","                # get info gain\n","                infoGain = self.getInfoGain(y, yLeft, yRight)\n","                # update split\n","                if infoGain > bestInfoGain:\n","                  bestInfoGain = infoGain\n","                  splitDetails = {\n","                    \"infoGain\" : infoGain, \n","                    \"leftTrainingSet\": left_training_set, \n","                    \"rightTrainingSet\": right_training_set, \n","                    \"attributeIndex\" : feature, \n","                    \"threshold\": thershold\n","                    }\n","\n","          # check if node is impure and recurse\n","          if splitDetails[\"infoGain\"] != None and splitDetails[\"infoGain\"] > 0:\n","            # repeat for left side\n","            left = self.learn(splitDetails[\"leftTrainingSet\"])\n","            # repeat for left side\n","            right = self.learn(splitDetails[\"rightTrainingSet\"])\n","            # return new node\n","            return Node(left, right, splitDetails[\"attributeIndex\"], splitDetails[\"threshold\"])\n","        \n","        # if the control reaches here, the node under consideration is a leaf node\n","        zeros = np.shape(y[y == 0])[0]\n","        ones = np.shape(y[y == 1])[0]\n","        if zeros > ones:\n","          return Node(label=0, isLeaf=True)\n","        else:\n","          return Node(label=1, isLeaf=True)\n","\n","    def classify(self, root, test_instance):\n","        if root.isLeaf == True:\n","          return root.label\n","        if test_instance[root.attribute_index] <= root.threshold:\n","          return self.classify(root.left, test_instance)\n","        else:\n","          return self.classify(root.right, test_instance)\n","\n","\n","def run_decision_tree():\n","\n","    # Load data set\n","    # data_set_path = \"/content/drive/MyDrive/Acad_Stuff/Course Work/Semester 5/FoML/Ass1/resources/wine-dataset.csv\"\n","    data_set_path = \"wine-dataset.csv\"\n","    # with open(\"wine-dataset.csv\") as f:\n","    with open(data_set_path) as f:\n","        next(f, None)\n","        data = [tuple(line) for line in csv.reader(f, delimiter=\",\")]\n","    print(\"Number of records: %d\" % len(data))\n","    \n","    # Split training/test sets\n","    # You need to modify the following code for cross validation.\n","    K = 10\n","    training_set = [x for i, x in enumerate(data) if i % K != 9]\n","    test_set = [x for i, x in enumerate(data) if i % K == 9]\n","\n","    training_set = np.asarray(training_set, dtype=np.float32)\n","    test_set = np.asarray(test_set, dtype=np.float32)\n","\n","    # print(np.shape(training_set))\n","    \n","    tree = DecisionTree()\n","    root = Node()\n","    # Construct a tree using training set\n","    root = tree.learn(training_set)\n","\n","    # Classify the test set using the tree we just constructed\n","    results = []\n","    for instance in test_set:\n","        result = tree.classify(root, instance[:-1])\n","        # print(result)\n","        results.append( result == int(instance[-1]))\n","\n","    \n","    # Accuracy\n","    accuracy = float(results.count(True))/float(len(results))\n","    print(\"accuracy: %.4f\" % accuracy)       \n","    \n","\n","    # Writing results to a file (DO NOT CHANGE)\n","    f = open(myname+\"result.txt\", \"w\")\n","    f.write(\"accuracy: %.4f\" % accuracy)\n","    f.close()\n","\n","\n","if __name__ == \"__main__\":\n","    run_decision_tree()"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of records: 4898\n","accuracy: 0.8221\n"]}]},{"cell_type":"markdown","metadata":{"id":"S3h_pMVz8y5j"},"source":["## (b)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-C6S8dBc84Ot","executionInfo":{"status":"ok","timestamp":1632496496088,"user_tz":-330,"elapsed":82677,"user":{"displayName":"SOUMI CHAKRABORTY","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09195013844947693927"}},"outputId":"89f19986-ab09-4fe0-8eb6-6b0c62865038"},"source":["# FoML Assign 1 Code Skeleton\n","# Please use this outline to implement your decision tree. You can add any code around this.\n","\n","import csv\n","import numpy as np\n","\n","# Enter You Name Here\n","myname = \"Soumi-Chakraborty\"\n","\n","class Node():\n","  def __init__(self, left=None, right=None, attribute_index=None, threshold=None, label=None, isLeaf=False):\n","    self.left = left\n","    self.right = right\n","    self.attribute_index = attribute_index\n","    self.threshold = threshold\n","    self.label = label\n","    self.isLeaf = isLeaf\n","\n","# Implement your decision tree below\n","class DecisionTree():\n","    \n","\n","    def getInfoGain(self, node, left, right):\n","      infoGain = self.entropy(node) - (np.shape(left)[0] / np.shape(node)[0]) * self.entropy(left) - (np.shape(right)[0] / np.shape(node)[0]) * self.entropy(right)  \n","      return infoGain\n","\n","    def entropy(self, y):\n","      #  since type can only be 0 or 1\n","      p0 = np.shape(y[y == 0])[0]/np.shape(y)[0]\n","      p1 = np.shape(y[y == 1])[0]/np.shape(y)[0]\n","      if p0 == 0 or p1 == 0:\n","        return 0\n","      entropy = -p0 * np.log2(p0) - p1 * np.log2(p1)\n","      return entropy\n","      \n","\n","    def learn(self, training_set):\n","        X = training_set[:, :-1] # all rows, all columns except the last\n","        y = training_set[:, -1]  # all rows, only the last column\n","        y = y.reshape(y.shape[0], 1)\n","\n","        # purity check\n","        check = y[y == 1].shape[0]\n","        \n","        if check != 0 and check != y.shape[0]:\n","          # get best split\n","          splitDetails = {\n","              \"infoGain\" : None, \n","              \"leftTrainingSet\": None, \n","              \"rightTrainingSet\": None, \n","              \"attributeIndex\" : None, \n","              \"threshold\": None\n","          }\n","          \n","          # check over all the attributes\n","          bestInfoGain = -float(\"inf\")\n","          for feature in range(X.shape[1]):\n","            values = training_set[:, feature] \n","            thersholds = np.unique(values)\n","            # compare all possible splits and choose the best one\n","            for thershold in thersholds:\n","              # divide the training set based on the threshold\n","              left_training_set = training_set[np.where(training_set[:, feature] <= thershold)]\n","              right_training_set = training_set[np.where(training_set[:, feature] > thershold)]\n","              if np.shape(left_training_set)[0] > 0 and np.shape(right_training_set)[0] > 0:\n","                yLeft = left_training_set[:, -1].reshape(np.shape(left_training_set)[0], 1)\n","                yRight = right_training_set[:, -1].reshape(np.shape(right_training_set)[0], 1)\n","\n","                # get info gain\n","                infoGain = self.getInfoGain(y, yLeft, yRight)\n","                # update split\n","                if infoGain > bestInfoGain:\n","                  bestInfoGain = infoGain\n","                  splitDetails = {\n","                    \"infoGain\" : infoGain, \n","                    \"leftTrainingSet\": left_training_set, \n","                    \"rightTrainingSet\": right_training_set, \n","                    \"attributeIndex\" : feature, \n","                    \"threshold\": thershold\n","                    }\n","\n","          # check if node is impure \n","          if splitDetails[\"infoGain\"] != None and splitDetails[\"infoGain\"] > 0:\n","            # repeat for left side\n","            left = self.learn(splitDetails[\"leftTrainingSet\"])\n","            # repeat for left side\n","            right = self.learn(splitDetails[\"rightTrainingSet\"])\n","            # return new node\n","            return Node(left, right, splitDetails[\"attributeIndex\"], splitDetails[\"threshold\"])\n","        \n","        # if the control reaches here, the node under consideration is a leaf node\n","        zeros = np.shape(y[y == 0])[0]\n","        ones = np.shape(y[y == 1])[0]\n","        if zeros > ones:\n","          return Node(label=0, isLeaf=True)\n","        else:\n","          return Node(label=1, isLeaf=True)\n","\n","    def classify(self, root, test_instance):\n","        if root.isLeaf == True:\n","          return root.label\n","        if test_instance[root.attribute_index] <= root.threshold:\n","          return self.classify(root.left, test_instance)\n","        else:\n","          return self.classify(root.right, test_instance)\n","\n","def run_decision_tree():\n","\n","    # Load data set\n","    # data_set_path = \"/content/drive/MyDrive/Acad_Stuff/Course Work/Semester 5/FoML/Ass1/resources/wine-dataset.csv\"\n","    data_set_path = \"wine-dataset.csv\"\n","\n","    # with open(\"wine-dataset.csv\") as f:\n","    with open(data_set_path) as f:\n","        next(f, None)\n","        data = [tuple(line) for line in csv.reader(f, delimiter=\",\")]\n","    print(\"Number of records: %d\" % len(data))\n","    \n","    # Split training/test sets\n","    # You need to modify the following code for cross validation.\n","    K = 10\n","    finalAccuracy = 0\n","    for j in range(K):\n","      training_set = [x for i, x in enumerate(data) if i % K != j]\n","      test_set = [x for i, x in enumerate(data) if i % K == j]\n","\n","      training_set = np.asarray(training_set, dtype=np.float32)\n","      test_set = np.asarray(test_set, dtype=np.float32)\n","\n","      # print(np.shape(training_set), np.shape(test_set))\n","      \n","      tree = DecisionTree()\n","      root = Node()\n","      # Construct a tree using training set\n","      root = tree.learn(training_set)\n","\n","      # Classify the test set using the tree we just constructed\n","      results = []\n","      for instance in test_set:\n","          result = tree.classify(root, instance[:-1])\n","          # print(result)\n","          results.append( result == int(instance[-1]))\n","\n","      \n","      # Accuracy\n","      accuracy = float(results.count(True))/float(len(results))\n","      print(\"accuracy: %.4f\" % accuracy)\n","      finalAccuracy = finalAccuracy + accuracy\n","\n","\n","    finalAccuracy = finalAccuracy/K\n","    accuracy = finalAccuracy\n","    print(\"final accuracy: %.4f\" % accuracy)\n","    \n","\n","    # Writing results to a file (DO NOT CHANGE)\n","    f = open(myname+\"result.txt\", \"w\")\n","    f.write(\"accuracy: %.4f\" % finalAccuracy)\n","    # f.write(\"final accuracy: %.4f\" % finalAccuracy)\n","    f.close()\n","\n","\n","if __name__ == \"__main__\":\n","    run_decision_tree()"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of records: 4898\n","accuracy: 0.8388\n","accuracy: 0.8633\n","accuracy: 0.8367\n","accuracy: 0.8408\n","accuracy: 0.8551\n","accuracy: 0.8041\n","accuracy: 0.8347\n","accuracy: 0.8429\n","accuracy: 0.8425\n","accuracy: 0.8221\n","final accuracy: 0.8381\n"]}]},{"cell_type":"markdown","metadata":{"id":"liuRZFIl81ID"},"source":["## (c)"]},{"cell_type":"markdown","metadata":{"id":"KVuezB6WX3JR"},"source":["### Improvement method 1: Gini Index"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XZmjI8RaWqmh","executionInfo":{"status":"ok","timestamp":1632496685677,"user_tz":-330,"elapsed":79115,"user":{"displayName":"SOUMI CHAKRABORTY","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09195013844947693927"}},"outputId":"9f9b22c9-3d24-4857-d69e-59c21f2fd388"},"source":["# FoML Assign 1 Code Skeleton\n","# Please use this outline to implement your decision tree. You can add any code around this.\n","\n","import csv\n","import numpy as np\n","\n","# Enter You Name Here\n","myname = \"Soumi-Chakraborty\"\n","\n","class Node():\n","  def __init__(self, left=None, right=None, attribute_index=None, threshold=None, label=None, isLeaf=False):\n","    self.left = left\n","    self.right = right\n","    self.attribute_index = attribute_index\n","    self.threshold = threshold\n","    self.label = label\n","    self.isLeaf = isLeaf\n","\n","# Implement your decision tree below\n","class DecisionTree():\n","\n","    def getInfoGain(self, node, left, right):\n","      # infoGain = self.entropy(node) - (len(left) / len(node)) * self.entropy(left) - (len(right) / len(node)) * self.entropy(right) \n","      infoGain = self.giniIndex(node) - ((np.shape(left)[0] / np.shape(node)[0]) * self.giniIndex(left) + (np.shape(right)[0] / np.shape(node)[0]) * self.giniIndex(right))\n","\n","      return infoGain\n","\n","    def giniIndex(self, y):\n","      #  since type can only be 0 or 1\n","      p0 = np.shape(y[y == 0])[0]/np.shape(y)[0]\n","      p1 = np.shape(y[y == 1])[0]/np.shape(y)[0]\n","      gini = 1 - p0**2 - p1**2\n","      return gini\n","      \n","\n","    def learn(self, training_set, depth):\n","        # implement this function\n","        X = training_set[:, :-1] # all rows, all columns except the last\n","        y = training_set[:, -1]  # all rows, only the last column\n","        y = y.reshape(y.shape[0], 1)\n","\n","        # purity check\n","        check = y[y == 1].shape[0]\n","        \n","        # # split if max depth not reached\n","        if check != 0 and check != y.shape[0]:\n","          # get best split\n","          splitDetails = {\n","              \"infoGain\" : None, \n","              \"leftTrainingSet\": None, \n","              \"rightTrainingSet\": None, \n","              \"attributeIndex\" : None, \n","              \"threshold\": None\n","          }\n","          \n","          # check over all the attributes\n","          bestInfoGain = -float(\"inf\")\n","          for feature in range(X.shape[1]):\n","            values = training_set[:, feature] \n","            thersholds = np.unique(values)\n","            # compare all possible splits and choose the best one\n","            for thershold in thersholds:\n","              # divide the training set based on the threshold\n","              left_training_set = training_set[np.where(training_set[:, feature] <= thershold)]\n","              right_training_set = training_set[np.where(training_set[:, feature] > thershold)]\n","              if np.shape(left_training_set)[0] > 0 and np.shape(right_training_set)[0] > 0:\n","                yLeft = left_training_set[:, -1].reshape(np.shape(left_training_set)[0], 1)\n","                yRight = right_training_set[:, -1].reshape(np.shape(right_training_set)[0], 1)\n","\n","                # get info gain\n","                infoGain = self.getInfoGain(y, yLeft, yRight)\n","                # update split\n","                if infoGain > bestInfoGain:\n","                  bestInfoGain = infoGain\n","                  splitDetails = {\n","                    \"infoGain\" : infoGain, \n","                    \"leftTrainingSet\": left_training_set, \n","                    \"rightTrainingSet\": right_training_set, \n","                    \"attributeIndex\" : feature, \n","                    \"threshold\": thershold\n","                    }\n","\n","          # check if node is impure \n","          if splitDetails[\"infoGain\"] != None and splitDetails[\"infoGain\"] > 0:\n","            # repeat for left side\n","            left = self.learn(splitDetails[\"leftTrainingSet\"], depth + 1)\n","            # repeat for left side\n","            right = self.learn(splitDetails[\"rightTrainingSet\"], depth + 1)\n","            # return new node\n","            return Node(left, right, splitDetails[\"attributeIndex\"], splitDetails[\"threshold\"])\n","        \n","        # if the control reaches here, the node under consideration is a leaf node\n","        zeros = np.shape(y[y == 0])[0]\n","        ones = np.shape(y[y == 1])[0]\n","        if zeros > ones:\n","          return Node(label=0, isLeaf=True)\n","        else:\n","          return Node(label=1, isLeaf=True)\n","\n","    def classify(self, root, test_instance):\n","        result = 0 # baseline: always classifies as 0\n","        if root.isLeaf == True:\n","          return root.label\n","        if test_instance[root.attribute_index] <= root.threshold:\n","          return self.classify(root.left, test_instance)\n","        else:\n","          return self.classify(root.right, test_instance)\n","\n","        # return result\n","\n","def run_decision_tree():\n","\n","    # Load data set\n","    # data_set_path = \"/content/drive/MyDrive/Acad_Stuff/Course Work/Semester 5/FoML/Ass1/resources/wine-dataset.csv\"\n","    data_set_path = \"wine-dataset.csv\"\n","\n","    # with open(\"wine-dataset.csv\") as f:\n","    with open(data_set_path) as f:\n","        next(f, None)\n","        data = [tuple(line) for line in csv.reader(f, delimiter=\",\")]\n","    print(\"Number of records: %d\" % len(data))\n","    \n","    # Split training/test sets\n","    # You need to modify the following code for cross validation.\n","    K = 10\n","    finalAccuracy = 0\n","    for j in range(K):\n","      training_set = [x for i, x in enumerate(data) if i % K != j]\n","      test_set = [x for i, x in enumerate(data) if i % K == j]\n","\n","      training_set = np.asarray(training_set, dtype=np.float32)\n","      test_set = np.asarray(test_set, dtype=np.float32)\n","\n","\n","      # print(np.shape(training_set), np.shape(test_set))\n","      \n","      tree = DecisionTree()\n","      root = Node()\n","      # Construct a tree using training set\n","      root = tree.learn(training_set, 0)\n","\n","      # Classify the test set using the tree we just constructed\n","      results = []\n","      for instance in test_set:\n","          result = tree.classify(root, instance[:-1])\n","          # print(result)\n","          results.append( result == int(instance[-1]))\n","\n","      \n","      # Accuracy\n","      accuracy = float(results.count(True))/float(len(results))\n","      print(\"accuracy: %.4f\" % accuracy)\n","      finalAccuracy = finalAccuracy + accuracy\n","\n","\n","    finalAccuracy = finalAccuracy/K\n","    accuracy = finalAccuracy\n","    print(\"final accuracy: %.4f\" % accuracy)\n","    \n","\n","    # Writing results to a file (DO NOT CHANGE)\n","    f = open(myname+\"result.txt\", \"w\")\n","    f.write(\"accuracy: %.4f\" % finalAccuracy)\n","    # f.write(\"final accuracy: %.4f\" % finalAccuracy)\n","    f.close()\n","\n","\n","if __name__ == \"__main__\":\n","    run_decision_tree()"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of records: 4898\n","accuracy: 0.8571\n","accuracy: 0.8633\n","accuracy: 0.8286\n","accuracy: 0.8469\n","accuracy: 0.8571\n","accuracy: 0.8306\n","accuracy: 0.8286\n","accuracy: 0.8612\n","accuracy: 0.8446\n","accuracy: 0.8303\n","final accuracy: 0.8448\n"]}]},{"cell_type":"markdown","metadata":{"id":"tF44Qfn2YXCi"},"source":["### Improvement method 2: Pruning"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jgU_MSCEbN5F","executionInfo":{"status":"ok","timestamp":1632498167043,"user_tz":-330,"elapsed":80187,"user":{"displayName":"SOUMI CHAKRABORTY","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09195013844947693927"}},"outputId":"5bd9c3c4-8be3-4319-9e92-2960849d82b3"},"source":["# FoML Assign 1 Code Skeleton\n","# Please use this outline to implement your decision tree. You can add any code around this.\n","\n","import csv\n","import numpy as np\n","\n","# Enter You Name Here\n","myname = \"Soumi-Chakraborty\"\n","MAX_DEPTH = 12\n","\n","class Node():\n","  def __init__(self, left=None, right=None, attribute_index=None, threshold=None, infoGain=None, label=None, depth=None, isLeaf=False):\n","    self.left = left\n","    self.right = right\n","    self.attribute_index = attribute_index\n","    self.threshold = threshold\n","    self.infoGain = infoGain\n","    self.label = label\n","    self.depth = depth\n","    self.isLeaf = isLeaf\n","\n","# Implement your decision tree below\n","class DecisionTree():\n","\n","    def getInfoGain(self, node, left, right):\n","      infoGain = self.giniIndex(node) - ((np.shape(left)[0] / np.shape(node)[0]) * self.giniIndex(left) + (np.shape(right)[0] / np.shape(node)[0]) * self.giniIndex(right))\n","      return infoGain\n","\n","    def giniIndex(self, y):\n","      #  since type can only be 0 or 1\n","      p0 = np.shape(y[y == 0])[0]/np.shape(y)[0]\n","      p1 = np.shape(y[y == 1])[0]/np.shape(y)[0]\n","      gini = 1 - p0**2 - p1**2\n","      return gini\n","      \n","\n","    def learn(self, training_set, depth):\n","        # implement this function\n","        X = training_set[:, :-1] # all rows, all columns except the last\n","        y = training_set[:, -1]  # all rows, only the last column\n","        y = y.reshape(y.shape[0], 1)\n","\n","        # purity check\n","        check = y[y == 1].shape[0]\n","        \n","        # # split if max depth not reached\n","        if check != 0 and check != y.shape[0]:\n","          # get best split\n","          splitDetails = {\n","              \"infoGain\" : None, \n","              \"leftTrainingSet\": None, \n","              \"rightTrainingSet\": None, \n","              \"attributeIndex\" : None, \n","              \"threshold\": None\n","          }\n","          \n","          # check over all the attributes\n","          bestInfoGain = -float(\"inf\")\n","          for feature in range(X.shape[1]):\n","            values = training_set[:, feature] \n","            thersholds = np.unique(values)\n","            # compare all possible splits and choose the best one\n","            for thershold in thersholds:\n","              # divide the training set based on the threshold\n","              left_training_set = training_set[np.where(training_set[:, feature] <= thershold)]\n","              right_training_set = training_set[np.where(training_set[:, feature] > thershold)]\n","              if np.shape(left_training_set)[0] > 0 and np.shape(right_training_set)[0] > 0:\n","                yLeft = left_training_set[:, -1].reshape(np.shape(left_training_set)[0], 1)\n","                yRight = right_training_set[:, -1].reshape(np.shape(right_training_set)[0], 1)\n","\n","                # get info gain\n","                infoGain = self.getInfoGain(y, yLeft, yRight)\n","                # print(infoGain)\n","                # update split\n","                if infoGain > bestInfoGain:\n","                  bestInfoGain = infoGain\n","                  splitDetails = {\n","                    \"infoGain\" : infoGain, \n","                    \"leftTrainingSet\": left_training_set, \n","                    \"rightTrainingSet\": right_training_set, \n","                    \"attributeIndex\" : feature, \n","                    \"threshold\": thershold\n","                    }\n","\n","          # check if node is impure \n","\n","          if splitDetails[\"infoGain\"] != None and splitDetails[\"infoGain\"] > 0:\n","            # repeat for left side\n","            left = self.learn(splitDetails[\"leftTrainingSet\"], depth + 1)\n","            # repeat for left side\n","            right = self.learn(splitDetails[\"rightTrainingSet\"], depth + 1)\n","            \n","            # storing label of each node so we don't have to repeat the steps while pruning\n","            label = 0\n","            zeros = np.shape(y[y == 0])[0]\n","            ones = np.shape(y[y == 1])[0]\n","            if zeros > ones:\n","              label = 0\n","            else:\n","              label = 1\n","            \n","            # return new node\n","            return Node(left, right, splitDetails[\"attributeIndex\"], splitDetails[\"threshold\"], splitDetails[\"infoGain\"], label, depth)\n","        \n","        # if the control reaches here, the node under consideration is a leaf node\n","        zeros = np.shape(y[y == 0])[0]\n","        ones = np.shape(y[y == 1])[0]\n","        if zeros > ones:\n","          # print(\"0\")\n","          return Node(label=0, isLeaf=True)\n","        else:\n","          # print(\"1\")\n","          return Node(label=1, isLeaf=True)\n","\n","    def classify(self, root, test_instance):\n","        result = 0 # baseline: always classifies as 0\n","        if root.isLeaf == True:\n","          return root.label\n","        if test_instance[root.attribute_index] <= root.threshold:\n","          return self.classify(root.left, test_instance)\n","        else:\n","          return self.classify(root.right, test_instance)\n","\n","        # return result\n","\n","    # top-down post-pruning\n","    def prune(self, root):\n","      if (root.isLeaf == False and root.depth <= MAX_DEPTH):\n","        self.prune(root.left)\n","        self.prune(root.right)\n","      elif (root.isLeaf == False and root.infoGain * 100 >= 1):\n","        self.prune(root.left)\n","        self.prune(root.right)\n","      else:\n","        root.isLeaf = True\n","        root.left = None\n","        root.right = None\n","        root.infoGain = None\n","\n","\n","def run_decision_tree():\n","\n","    # Load data set\n","    # data_set_path = \"/content/drive/MyDrive/Acad_Stuff/Course Work/Semester 5/FoML/Ass1/resources/wine-dataset.csv\"\n","    data_set_path = \"wine-dataset.csv\"\n","\n","    # with open(\"wine-dataset.csv\") as f:\n","    with open(data_set_path) as f:\n","        next(f, None)\n","        data = [tuple(line) for line in csv.reader(f, delimiter=\",\")]\n","    print(\"Number of records: %d\" % len(data))\n","    \n","    # Split training/test sets\n","    # You need to modify the following code for cross validation.\n","    K = 10\n","    finalAccuracy = 0\n","    for j in range(K):\n","      training_set = [x for i, x in enumerate(data) if i % K != j]\n","      test_set = [x for i, x in enumerate(data) if i % K == j]\n","\n","      training_set = np.asarray(training_set, dtype=np.float32)\n","      test_set = np.asarray(test_set, dtype=np.float32)\n","\n","\n","      # print(np.shape(training_set), np.shape(test_set))\n","      \n","      tree = DecisionTree()\n","      root = Node()\n","      # Construct a tree using training set\n","      root = tree.learn(training_set, 0)\n","      tree.prune(root)\n","      # print(root.infoGain)\n","\n","      # Classify the test set using the tree we just constructed\n","      results = []\n","      for instance in test_set:\n","          result = tree.classify(root, instance[:-1])\n","          # print(result)\n","          results.append( result == int(instance[-1]))\n","\n","      \n","      # Accuracy\n","      accuracy = float(results.count(True))/float(len(results))\n","      print(\"accuracy: %.4f\" % accuracy)\n","      finalAccuracy = finalAccuracy + accuracy\n","\n","\n","    finalAccuracy = finalAccuracy/K\n","    accuracy = finalAccuracy\n","    print(\"final accuracy: %.4f\" % accuracy)\n","    \n","\n","    # Writing results to a file (DO NOT CHANGE)\n","    f = open(myname+\"result.txt\", \"w\")\n","    f.write(\"accuracy: %.4f\" % finalAccuracy)\n","    # f.write(\"final accuracy: %.4f\" % finalAccuracy)\n","    f.close()\n","\n","\n","if __name__ == \"__main__\":\n","    run_decision_tree()"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of records: 4898\n","accuracy: 0.8571\n","accuracy: 0.8633\n","accuracy: 0.8286\n","accuracy: 0.8469\n","accuracy: 0.8571\n","accuracy: 0.8265\n","accuracy: 0.8286\n","accuracy: 0.8653\n","accuracy: 0.8466\n","accuracy: 0.8303\n","final accuracy: 0.8450\n"]}]},{"cell_type":"markdown","metadata":{"id":"K-MaZu0uSiKN"},"source":["Accuracy without 10-fold cross validation: 82.21%\\\n","Accuracy with 10-fold cross validation: 83.81%\\\n","Accuracy with just Gini index instead of entropy: 84.48%\\\n","Accuracy with Gini index and pruning: 84.50%\n","\n","The improvement methods I implemented were to replace entropy with the gini index, and then do top-down pruning alongside gini. Calculating the gini index is less computationally expensive than calculating the entropy. The top-down pruning only starts to prune the tree after a certain depth (arbritarity chosen to be 12) and prunes if the information gain of any node is below a  particular thershold (arbitrarily chosen to be 1%) and these combined methods led to an increase in accuracy because it removes some overfitting subtrees.\n"]}]}